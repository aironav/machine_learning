{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confident-mistake",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "- ### imputation\n",
    "    - missing data handling\n",
    "        - numerical data imputation\n",
    "            - mean value is filled in missing places\n",
    "        - categorical data imputation\n",
    "            - frequency (mode) value is filled in missing places\n",
    "- ### encoding\n",
    "    - change ur categorical(non numerical data) into numerical data\n",
    "        - Label Encoding\n",
    "            - if a column has 2 unique categorical values\n",
    "        - One Hot Encoding\n",
    "            - if a column has more than 2 categorical values\n",
    "        - Vectorization\n",
    "            - if you want to convert words to vector(numerical) format\n",
    "\n",
    "- ### scaling/normalizaton\n",
    "    - how to make the all the column to same scale so that ML algo can work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dangerous-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parliamentary-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "differential-domestic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'sample_data.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developed-treasurer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('datasets/sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "selected-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "manual-crawford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germany</td>\n",
       "      <td>49000.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spain</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>italy</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spain</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>italy</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spain</td>\n",
       "      <td>31000.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>germany</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spain</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>italy</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>italy</td>\n",
       "      <td>340000.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>germany</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country    salary   age happy\n",
       "0   germany   49000.0  35.0   yes\n",
       "1     spain   10000.0  30.0    no\n",
       "2     italy  230000.0  39.0    no\n",
       "3     spain  200000.0  30.0   yes\n",
       "4     italy  300000.0  30.0   yes\n",
       "5     spain   31000.0  23.0    no\n",
       "6   germany       NaN  34.0   yes\n",
       "7     spain  400000.0   NaN   yes\n",
       "8     italy  200000.0  29.0    no\n",
       "9     italy  340000.0  35.0   yes\n",
       "10  germany   30000.0  25.0   yes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-beauty",
   "metadata": {},
   "source": [
    "IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "endangered-nudist",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mSimpleImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmissing_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0madd_indicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Imputation transformer for completing missing values.\n",
       "\n",
       "Read more in the :ref:`User Guide <impute>`.\n",
       "\n",
       ".. versionadded:: 0.20\n",
       "   `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n",
       "   estimator which is now removed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "missing_values : int, float, str, np.nan or None, default=np.nan\n",
       "    The placeholder for the missing values. All occurrences of\n",
       "    `missing_values` will be imputed. For pandas' dataframes with\n",
       "    nullable integer dtypes with missing values, `missing_values`\n",
       "    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n",
       "\n",
       "strategy : string, default='mean'\n",
       "    The imputation strategy.\n",
       "\n",
       "    - If \"mean\", then replace missing values using the mean along\n",
       "      each column. Can only be used with numeric data.\n",
       "    - If \"median\", then replace missing values using the median along\n",
       "      each column. Can only be used with numeric data.\n",
       "    - If \"most_frequent\", then replace missing using the most frequent\n",
       "      value along each column. Can be used with strings or numeric data.\n",
       "      If there is more than one such value, only the smallest is returned.\n",
       "    - If \"constant\", then replace missing values with fill_value. Can be\n",
       "      used with strings or numeric data.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "       strategy=\"constant\" for fixed value imputation.\n",
       "\n",
       "fill_value : string or numerical value, default=None\n",
       "    When strategy == \"constant\", fill_value is used to replace all\n",
       "    occurrences of missing_values.\n",
       "    If left to the default, fill_value will be 0 when imputing numerical\n",
       "    data and \"missing_value\" for strings or object data types.\n",
       "\n",
       "verbose : integer, default=0\n",
       "    Controls the verbosity of the imputer.\n",
       "\n",
       "copy : boolean, default=True\n",
       "    If True, a copy of X will be created. If False, imputation will\n",
       "    be done in-place whenever possible. Note that, in the following cases,\n",
       "    a new copy will always be made, even if `copy=False`:\n",
       "\n",
       "    - If X is not an array of floating values;\n",
       "    - If X is encoded as a CSR matrix;\n",
       "    - If add_indicator=True.\n",
       "\n",
       "add_indicator : boolean, default=False\n",
       "    If True, a :class:`MissingIndicator` transform will stack onto output\n",
       "    of the imputer's transform. This allows a predictive estimator\n",
       "    to account for missingness despite imputation. If a feature has no\n",
       "    missing values at fit/train time, the feature won't appear on\n",
       "    the missing indicator even if there are missing values at\n",
       "    transform/test time.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "statistics_ : array of shape (n_features,)\n",
       "    The imputation fill value for each feature.\n",
       "    Computing statistics can result in `np.nan` values.\n",
       "    During :meth:`transform`, features corresponding to `np.nan`\n",
       "    statistics will be discarded.\n",
       "\n",
       "indicator_ : :class:`~sklearn.impute.MissingIndicator`\n",
       "    Indicator used to add binary indicators for missing values.\n",
       "    ``None`` if add_indicator is False.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "IterativeImputer : Multivariate imputation of missing values.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.impute import SimpleImputer\n",
       ">>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
       ">>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
       "SimpleImputer()\n",
       ">>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
       ">>> print(imp_mean.transform(X))\n",
       "[[ 7.   2.   3. ]\n",
       " [ 4.   3.5  6. ]\n",
       " [10.   3.5  9. ]]\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Columns which only contained missing values at :meth:`fit` are discarded\n",
       "upon :meth:`transform` if strategy is not \"constant\".\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\xaidi\\appdata\\roaming\\python\\python39\\site-packages\\sklearn\\impute\\_base.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SimpleImputer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informative-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "charitable-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['salary','age']\n",
    "df[cols] = imputer.fit_transform(df[cols]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "minus-thong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germany</td>\n",
       "      <td>49000</td>\n",
       "      <td>35</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spain</td>\n",
       "      <td>10000</td>\n",
       "      <td>30</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>italy</td>\n",
       "      <td>230000</td>\n",
       "      <td>39</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spain</td>\n",
       "      <td>200000</td>\n",
       "      <td>30</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>italy</td>\n",
       "      <td>300000</td>\n",
       "      <td>30</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spain</td>\n",
       "      <td>31000</td>\n",
       "      <td>23</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>germany</td>\n",
       "      <td>179000</td>\n",
       "      <td>34</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spain</td>\n",
       "      <td>400000</td>\n",
       "      <td>31</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>italy</td>\n",
       "      <td>200000</td>\n",
       "      <td>29</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>italy</td>\n",
       "      <td>340000</td>\n",
       "      <td>35</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>germany</td>\n",
       "      <td>30000</td>\n",
       "      <td>25</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country  salary  age happy\n",
       "0   germany   49000   35   yes\n",
       "1     spain   10000   30    no\n",
       "2     italy  230000   39    no\n",
       "3     spain  200000   30   yes\n",
       "4     italy  300000   30   yes\n",
       "5     spain   31000   23    no\n",
       "6   germany  179000   34   yes\n",
       "7     spain  400000   31   yes\n",
       "8     italy  200000   29    no\n",
       "9     italy  340000   35   yes\n",
       "10  germany   30000   25   yes"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-structure",
   "metadata": {},
   "source": [
    "LABEL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "infrared-yahoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germany</td>\n",
       "      <td>49000</td>\n",
       "      <td>35</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spain</td>\n",
       "      <td>10000</td>\n",
       "      <td>30</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>italy</td>\n",
       "      <td>230000</td>\n",
       "      <td>39</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spain</td>\n",
       "      <td>200000</td>\n",
       "      <td>30</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>italy</td>\n",
       "      <td>300000</td>\n",
       "      <td>30</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spain</td>\n",
       "      <td>31000</td>\n",
       "      <td>23</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>germany</td>\n",
       "      <td>179000</td>\n",
       "      <td>34</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spain</td>\n",
       "      <td>400000</td>\n",
       "      <td>31</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>italy</td>\n",
       "      <td>200000</td>\n",
       "      <td>29</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>italy</td>\n",
       "      <td>340000</td>\n",
       "      <td>35</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>germany</td>\n",
       "      <td>30000</td>\n",
       "      <td>25</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country  salary  age happy\n",
       "0   germany   49000   35   yes\n",
       "1     spain   10000   30    no\n",
       "2     italy  230000   39    no\n",
       "3     spain  200000   30   yes\n",
       "4     italy  300000   30   yes\n",
       "5     spain   31000   23    no\n",
       "6   germany  179000   34   yes\n",
       "7     spain  400000   31   yes\n",
       "8     italy  200000   29    no\n",
       "9     italy  340000   35   yes\n",
       "10  germany   30000   25   yes"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "neither-saint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country     3\n",
       "salary     10\n",
       "age         8\n",
       "happy       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique() # we select happy col for label encoding coz of 2 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exact-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpyEncoder = LabelEncoder()\n",
    "df['happy'] = hpyEncoder.fit_transform(df['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legal-chambers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germany</td>\n",
       "      <td>49000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spain</td>\n",
       "      <td>10000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>italy</td>\n",
       "      <td>230000</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spain</td>\n",
       "      <td>200000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>italy</td>\n",
       "      <td>300000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spain</td>\n",
       "      <td>31000</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>germany</td>\n",
       "      <td>179000</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spain</td>\n",
       "      <td>400000</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>italy</td>\n",
       "      <td>200000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>italy</td>\n",
       "      <td>340000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>germany</td>\n",
       "      <td>30000</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country  salary  age  happy\n",
       "0   germany   49000   35      1\n",
       "1     spain   10000   30      0\n",
       "2     italy  230000   39      0\n",
       "3     spain  200000   30      1\n",
       "4     italy  300000   30      1\n",
       "5     spain   31000   23      0\n",
       "6   germany  179000   34      1\n",
       "7     spain  400000   31      1\n",
       "8     italy  200000   29      0\n",
       "9     italy  340000   35      1\n",
       "10  germany   30000   25      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-portsmouth",
   "metadata": {},
   "source": [
    "ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "meaning-prison",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.float64'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Encode categorical features as a one-hot numeric array.\n",
       "\n",
       "The input to this transformer should be an array-like of integers or\n",
       "strings, denoting the values taken on by categorical (discrete) features.\n",
       "The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
       "encoding scheme. This creates a binary column for each category and\n",
       "returns a sparse matrix or dense array (depending on the ``sparse``\n",
       "parameter)\n",
       "\n",
       "By default, the encoder derives the categories based on the unique values\n",
       "in each feature. Alternatively, you can also specify the `categories`\n",
       "manually.\n",
       "\n",
       "This encoding is needed for feeding categorical data to many scikit-learn\n",
       "estimators, notably linear models and SVMs with the standard kernels.\n",
       "\n",
       "Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
       "instead.\n",
       "\n",
       "Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
       "\n",
       ".. versionchanged:: 0.20\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "categories : 'auto' or a list of array-like, default='auto'\n",
       "    Categories (unique values) per feature:\n",
       "\n",
       "    - 'auto' : Determine categories automatically from the training data.\n",
       "    - list : ``categories[i]`` holds the categories expected in the ith\n",
       "      column. The passed categories should not mix strings and numeric\n",
       "      values within a single feature, and should be sorted in case of\n",
       "      numeric values.\n",
       "\n",
       "    The used categories can be found in the ``categories_`` attribute.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "drop : {'first', 'if_binary'} or a array-like of shape (n_features,),             default=None\n",
       "    Specifies a methodology to use to drop one of the categories per\n",
       "    feature. This is useful in situations where perfectly collinear\n",
       "    features cause problems, such as when feeding the resulting data\n",
       "    into a neural network or an unregularized regression.\n",
       "\n",
       "    However, dropping one category breaks the symmetry of the original\n",
       "    representation and can therefore induce a bias in downstream models,\n",
       "    for instance for penalized linear classification or regression models.\n",
       "\n",
       "    - None : retain all features (the default).\n",
       "    - 'first' : drop the first category in each feature. If only one\n",
       "      category is present, the feature will be dropped entirely.\n",
       "    - 'if_binary' : drop the first category in each feature with two\n",
       "      categories. Features with 1 or more than 2 categories are\n",
       "      left intact.\n",
       "    - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
       "      should be dropped.\n",
       "\n",
       "    .. versionchanged:: 0.23\n",
       "       Added option 'if_binary'.\n",
       "\n",
       "sparse : bool, default=True\n",
       "    Will return sparse matrix if set True else will return an array.\n",
       "\n",
       "dtype : number type, default=float\n",
       "    Desired dtype of output.\n",
       "\n",
       "handle_unknown : {'error', 'ignore'}, default='error'\n",
       "    Whether to raise an error or ignore if an unknown categorical feature\n",
       "    is present during transform (default is to raise). When this parameter\n",
       "    is set to 'ignore' and an unknown category is encountered during\n",
       "    transform, the resulting one-hot encoded columns for this feature\n",
       "    will be all zeros. In the inverse transform, an unknown category\n",
       "    will be denoted as None.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "categories_ : list of arrays\n",
       "    The categories of each feature determined during fitting\n",
       "    (in order of the features in X and corresponding with the output\n",
       "    of ``transform``). This includes the category specified in ``drop``\n",
       "    (if any).\n",
       "\n",
       "drop_idx_ : array of shape (n_features,)\n",
       "    - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n",
       "      to be dropped for each feature.\n",
       "    - ``drop_idx_[i] = None`` if no category is to be dropped from the\n",
       "      feature with index ``i``, e.g. when `drop='if_binary'` and the\n",
       "      feature isn't binary.\n",
       "    - ``drop_idx_ = None`` if all the transformed features will be\n",
       "      retained.\n",
       "\n",
       "    .. versionchanged:: 0.23\n",
       "       Added the possibility to contain `None` values.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "OrdinalEncoder : Performs an ordinal (integer)\n",
       "  encoding of the categorical features.\n",
       "sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n",
       "  dictionary items (also handles string-valued features).\n",
       "sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n",
       "  encoding of dictionary items or strings.\n",
       "LabelBinarizer : Binarizes labels in a one-vs-all\n",
       "  fashion.\n",
       "MultiLabelBinarizer : Transforms between iterable of\n",
       "  iterables and a multilabel format, e.g. a (samples x classes) binary\n",
       "  matrix indicating the presence of a class label.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Given a dataset with two features, we let the encoder find the unique\n",
       "values per feature and transform the data to a binary one-hot encoding.\n",
       "\n",
       ">>> from sklearn.preprocessing import OneHotEncoder\n",
       "\n",
       "One can discard categories not seen during `fit`:\n",
       "\n",
       ">>> enc = OneHotEncoder(handle_unknown='ignore')\n",
       ">>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
       ">>> enc.fit(X)\n",
       "OneHotEncoder(handle_unknown='ignore')\n",
       ">>> enc.categories_\n",
       "[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
       ">>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
       "array([[1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])\n",
       ">>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
       "array([['Male', 1],\n",
       "       [None, 2]], dtype=object)\n",
       ">>> enc.get_feature_names(['gender', 'group'])\n",
       "array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'],\n",
       "  dtype=object)\n",
       "\n",
       "One can always drop the first column for each feature:\n",
       "\n",
       ">>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
       ">>> drop_enc.categories_\n",
       "[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
       ">>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
       "array([[0., 0., 0.],\n",
       "       [1., 1., 0.]])\n",
       "\n",
       "Or drop a column for feature only having 2 categories:\n",
       "\n",
       ">>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
       ">>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0.]])\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\xaidi\\appdata\\roaming\\python\\python39\\site-packages\\sklearn\\preprocessing\\_encoders.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OneHotEncoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "answering-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryHotEnc = OneHotEncoder(drop='first')\n",
    "country_enc = countryHotEnc.fit_transform(df[['country']]).toarray() # 2 sqaure bracket for making data into 2d column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "latest-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_enc_countrydf = pd.DataFrame(country_enc,columns=['italy','spain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "typical-factory",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NDFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NDFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverify_integrity\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msort\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Series'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Concatenate pandas objects along a particular axis with optional set logic\n",
       "along the other axes.\n",
       "\n",
       "Can also add a layer of hierarchical indexing on the concatenation axis,\n",
       "which may be useful if the labels are the same (or overlapping) on\n",
       "the passed axis number.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "objs : a sequence or mapping of Series or DataFrame objects\n",
       "    If a mapping is passed, the sorted keys will be used as the `keys`\n",
       "    argument, unless it is passed, in which case the values will be\n",
       "    selected (see below). Any None objects will be dropped silently unless\n",
       "    they are all None in which case a ValueError will be raised.\n",
       "axis : {0/'index', 1/'columns'}, default 0\n",
       "    The axis to concatenate along.\n",
       "join : {'inner', 'outer'}, default 'outer'\n",
       "    How to handle indexes on other axis (or axes).\n",
       "ignore_index : bool, default False\n",
       "    If True, do not use the index values along the concatenation axis. The\n",
       "    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n",
       "    concatenating objects where the concatenation axis does not have\n",
       "    meaningful indexing information. Note the index values on the other\n",
       "    axes are still respected in the join.\n",
       "keys : sequence, default None\n",
       "    If multiple levels passed, should contain tuples. Construct\n",
       "    hierarchical index using the passed keys as the outermost level.\n",
       "levels : list of sequences, default None\n",
       "    Specific levels (unique values) to use for constructing a\n",
       "    MultiIndex. Otherwise they will be inferred from the keys.\n",
       "names : list, default None\n",
       "    Names for the levels in the resulting hierarchical index.\n",
       "verify_integrity : bool, default False\n",
       "    Check whether the new concatenated axis contains duplicates. This can\n",
       "    be very expensive relative to the actual data concatenation.\n",
       "sort : bool, default False\n",
       "    Sort non-concatenation axis if it is not already aligned when `join`\n",
       "    is 'outer'.\n",
       "    This has no effect when ``join='inner'``, which already preserves\n",
       "    the order of the non-concatenation axis.\n",
       "\n",
       "    .. versionchanged:: 1.0.0\n",
       "\n",
       "       Changed to not sort by default.\n",
       "\n",
       "copy : bool, default True\n",
       "    If False, do not copy data unnecessarily.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "object, type of objs\n",
       "    When concatenating all ``Series`` along the index (axis=0), a\n",
       "    ``Series`` is returned. When ``objs`` contains at least one\n",
       "    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n",
       "    the columns (axis=1), a ``DataFrame`` is returned.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "Series.append : Concatenate Series.\n",
       "DataFrame.append : Concatenate DataFrames.\n",
       "DataFrame.join : Join DataFrames using indexes.\n",
       "DataFrame.merge : Merge DataFrames by indexes or columns.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The keys, levels, and names arguments are all optional.\n",
       "\n",
       "A walkthrough of how this method fits in with other tools for combining\n",
       "pandas objects can be found `here\n",
       "<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Combine two ``Series``.\n",
       "\n",
       ">>> s1 = pd.Series(['a', 'b'])\n",
       ">>> s2 = pd.Series(['c', 'd'])\n",
       ">>> pd.concat([s1, s2])\n",
       "0    a\n",
       "1    b\n",
       "0    c\n",
       "1    d\n",
       "dtype: object\n",
       "\n",
       "Clear the existing index and reset it in the result\n",
       "by setting the ``ignore_index`` option to ``True``.\n",
       "\n",
       ">>> pd.concat([s1, s2], ignore_index=True)\n",
       "0    a\n",
       "1    b\n",
       "2    c\n",
       "3    d\n",
       "dtype: object\n",
       "\n",
       "Add a hierarchical index at the outermost level of\n",
       "the data with the ``keys`` option.\n",
       "\n",
       ">>> pd.concat([s1, s2], keys=['s1', 's2'])\n",
       "s1  0    a\n",
       "    1    b\n",
       "s2  0    c\n",
       "    1    d\n",
       "dtype: object\n",
       "\n",
       "Label the index keys you create with the ``names`` option.\n",
       "\n",
       ">>> pd.concat([s1, s2], keys=['s1', 's2'],\n",
       "...           names=['Series name', 'Row ID'])\n",
       "Series name  Row ID\n",
       "s1           0         a\n",
       "             1         b\n",
       "s2           0         c\n",
       "             1         d\n",
       "dtype: object\n",
       "\n",
       "Combine two ``DataFrame`` objects with identical columns.\n",
       "\n",
       ">>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n",
       "...                    columns=['letter', 'number'])\n",
       ">>> df1\n",
       "  letter  number\n",
       "0      a       1\n",
       "1      b       2\n",
       ">>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n",
       "...                    columns=['letter', 'number'])\n",
       ">>> df2\n",
       "  letter  number\n",
       "0      c       3\n",
       "1      d       4\n",
       ">>> pd.concat([df1, df2])\n",
       "  letter  number\n",
       "0      a       1\n",
       "1      b       2\n",
       "0      c       3\n",
       "1      d       4\n",
       "\n",
       "Combine ``DataFrame`` objects with overlapping columns\n",
       "and return everything. Columns outside the intersection will\n",
       "be filled with ``NaN`` values.\n",
       "\n",
       ">>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n",
       "...                    columns=['letter', 'number', 'animal'])\n",
       ">>> df3\n",
       "  letter  number animal\n",
       "0      c       3    cat\n",
       "1      d       4    dog\n",
       ">>> pd.concat([df1, df3], sort=False)\n",
       "  letter  number animal\n",
       "0      a       1    NaN\n",
       "1      b       2    NaN\n",
       "0      c       3    cat\n",
       "1      d       4    dog\n",
       "\n",
       "Combine ``DataFrame`` objects with overlapping columns\n",
       "and return only those that are shared by passing ``inner`` to\n",
       "the ``join`` keyword argument.\n",
       "\n",
       ">>> pd.concat([df1, df3], join=\"inner\")\n",
       "  letter  number\n",
       "0      a       1\n",
       "1      b       2\n",
       "0      c       3\n",
       "1      d       4\n",
       "\n",
       "Combine ``DataFrame`` objects horizontally along the x axis by\n",
       "passing in ``axis=1``.\n",
       "\n",
       ">>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n",
       "...                    columns=['animal', 'name'])\n",
       ">>> pd.concat([df1, df4], axis=1)\n",
       "  letter  number  animal    name\n",
       "0      a       1    bird   polly\n",
       "1      b       2  monkey  george\n",
       "\n",
       "Prevent the result from including duplicate index values with the\n",
       "``verify_integrity`` option.\n",
       "\n",
       ">>> df5 = pd.DataFrame([1], index=['a'])\n",
       ">>> df5\n",
       "   0\n",
       "a  1\n",
       ">>> df6 = pd.DataFrame([2], index=['a'])\n",
       ">>> df6\n",
       "   0\n",
       "a  2\n",
       ">>> pd.concat([df5, df6], verify_integrity=True)\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "ValueError: Indexes have overlapping values: ['a']\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\xaidi\\appdata\\roaming\\python\\python39\\site-packages\\pandas\\core\\reshape\\concat.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "serial-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([hot_enc_countrydf,df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "double-activity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>italy</th>\n",
       "      <th>spain</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230000</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31000</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179000</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>400000</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>340000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    italy  spain  salary  age  happy\n",
       "0     0.0    0.0   49000   35      1\n",
       "1     0.0    1.0   10000   30      0\n",
       "2     1.0    0.0  230000   39      0\n",
       "3     0.0    1.0  200000   30      1\n",
       "4     1.0    0.0  300000   30      1\n",
       "5     0.0    1.0   31000   23      0\n",
       "6     0.0    0.0  179000   34      1\n",
       "7     0.0    1.0  400000   31      1\n",
       "8     1.0    0.0  200000   29      0\n",
       "9     1.0    0.0  340000   35      1\n",
       "10    0.0    0.0   30000   25      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['country'],axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-rental",
   "metadata": {},
   "source": [
    "SCALING or NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eastern-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['italy', 'spain', 'salary', 'age']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = df.columns.tolist()[:-1]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "monetary-printer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Standardize features by removing the mean and scaling to unit variance\n",
       "\n",
       "The standard score of a sample `x` is calculated as:\n",
       "\n",
       "    z = (x - u) / s\n",
       "\n",
       "where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
       "and `s` is the standard deviation of the training samples or one if\n",
       "`with_std=False`.\n",
       "\n",
       "Centering and scaling happen independently on each feature by computing\n",
       "the relevant statistics on the samples in the training set. Mean and\n",
       "standard deviation are then stored to be used on later data using\n",
       ":meth:`transform`.\n",
       "\n",
       "Standardization of a dataset is a common requirement for many\n",
       "machine learning estimators: they might behave badly if the\n",
       "individual features do not more or less look like standard normally\n",
       "distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
       "\n",
       "For instance many elements used in the objective function of\n",
       "a learning algorithm (such as the RBF kernel of Support Vector\n",
       "Machines or the L1 and L2 regularizers of linear models) assume that\n",
       "all features are centered around 0 and have variance in the same\n",
       "order. If a feature has a variance that is orders of magnitude larger\n",
       "that others, it might dominate the objective function and make the\n",
       "estimator unable to learn from other features correctly as expected.\n",
       "\n",
       "This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
       "`with_mean=False` to avoid breaking the sparsity structure of the data.\n",
       "\n",
       "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "copy : bool, default=True\n",
       "    If False, try to avoid a copy and do inplace scaling instead.\n",
       "    This is not guaranteed to always work inplace; e.g. if the data is\n",
       "    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
       "    returned.\n",
       "\n",
       "with_mean : bool, default=True\n",
       "    If True, center the data before scaling.\n",
       "    This does not work (and will raise an exception) when attempted on\n",
       "    sparse matrices, because centering them entails building a dense\n",
       "    matrix which in common use cases is likely to be too large to fit in\n",
       "    memory.\n",
       "\n",
       "with_std : bool, default=True\n",
       "    If True, scale the data to unit variance (or equivalently,\n",
       "    unit standard deviation).\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "scale_ : ndarray of shape (n_features,) or None\n",
       "    Per feature relative scaling of the data to achieve zero mean and unit\n",
       "    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
       "    variance is zero, we can't achieve unit variance, and the data is left\n",
       "    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
       "    when `with_std=False`.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *scale_*\n",
       "\n",
       "mean_ : ndarray of shape (n_features,) or None\n",
       "    The mean value for each feature in the training set.\n",
       "    Equal to ``None`` when ``with_mean=False``.\n",
       "\n",
       "var_ : ndarray of shape (n_features,) or None\n",
       "    The variance for each feature in the training set. Used to compute\n",
       "    `scale_`. Equal to ``None`` when ``with_std=False``.\n",
       "\n",
       "n_samples_seen_ : int or ndarray of shape (n_features,)\n",
       "    The number of samples processed by the estimator for each feature.\n",
       "    If there are no missing samples, the ``n_samples_seen`` will be an\n",
       "    integer, otherwise it will be an array of dtype int. If\n",
       "    `sample_weights` are used it will be a float (if no missing data)\n",
       "    or an array of dtype float that sums the weights seen so far.\n",
       "    Will be reset on new calls to fit, but increments across\n",
       "    ``partial_fit`` calls.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
       ">>> scaler = StandardScaler()\n",
       ">>> print(scaler.fit(data))\n",
       "StandardScaler()\n",
       ">>> print(scaler.mean_)\n",
       "[0.5 0.5]\n",
       ">>> print(scaler.transform(data))\n",
       "[[-1. -1.]\n",
       " [-1. -1.]\n",
       " [ 1.  1.]\n",
       " [ 1.  1.]]\n",
       ">>> print(scaler.transform([[2, 2]]))\n",
       "[[3. 3.]]\n",
       "\n",
       "See Also\n",
       "--------\n",
       "scale : Equivalent function without the estimator API.\n",
       "\n",
       ":class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
       "    correlation across features with 'whiten=True'.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
       "transform.\n",
       "\n",
       "We use a biased estimator for the standard deviation, equivalent to\n",
       "`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
       "affect model performance.\n",
       "\n",
       "For a comparison of the different scalers, transformers, and normalizers,\n",
       "see :ref:`examples/preprocessing/plot_all_scaling.py\n",
       "<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\xaidi\\appdata\\roaming\\python\\python39\\site-packages\\sklearn\\preprocessing\\_data.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "StandardScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "julian-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = StandardScaler()\n",
    "df[cols] = scaling.fit_transform(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "protective-baker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>italy</th>\n",
       "      <th>spain</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.755929</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>-1.009405</td>\n",
       "      <td>0.911147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.755929</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>-1.312226</td>\n",
       "      <td>-0.227787</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.322876</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.395997</td>\n",
       "      <td>1.822294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.755929</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>0.163058</td>\n",
       "      <td>-0.227787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.322876</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.939523</td>\n",
       "      <td>-0.227787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.755929</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>-1.149169</td>\n",
       "      <td>-1.822294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.755929</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.755929</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>1.715988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.322876</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.163058</td>\n",
       "      <td>-0.455573</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.322876</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>1.250109</td>\n",
       "      <td>0.911147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.755929</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>-1.156933</td>\n",
       "      <td>-1.366720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       italy     spain    salary       age  happy\n",
       "0  -0.755929 -0.755929 -1.009405  0.911147      1\n",
       "1  -0.755929  1.322876 -1.312226 -0.227787      0\n",
       "2   1.322876 -0.755929  0.395997  1.822294      0\n",
       "3  -0.755929  1.322876  0.163058 -0.227787      1\n",
       "4   1.322876 -0.755929  0.939523 -0.227787      1\n",
       "5  -0.755929  1.322876 -1.149169 -1.822294      0\n",
       "6  -0.755929 -0.755929  0.000000  0.683360      1\n",
       "7  -0.755929  1.322876  1.715988  0.000000      1\n",
       "8   1.322876 -0.755929  0.163058 -0.455573      0\n",
       "9   1.322876 -0.755929  1.250109  0.911147      1\n",
       "10 -0.755929 -0.755929 -1.156933 -1.366720      1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-criminal",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-belarus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
